{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mishra-shriya/mishra-shriya-DataScience-GenAI-Submissions/blob/main/Exercise_8/8_01_Transformer_Backbone_and_Finetuning_COMPLETED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "s_TET4SyE2wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.01 Transformers: Backbone and Fine-tuning\n",
        "\n",
        "## DO THIS FIRST!!\n",
        "You will need to use a Hugging Face token to make this work. Follow these steps:\n",
        "1. Got to https://huggingface.co/\n",
        "2. Click \"Sign Up\" in the top right corner.\n",
        "3. Do the usual account sign up steps.\n",
        "4. Make sure you go to your email and click on the link to confirm your account.\n",
        "5. Once logged-in, click on your icon in the top right corner and select \"Access tokens\" (right at the bottom of the menu).\n",
        "6. Click \"+ Create new token\".\n",
        "7. Give your token a name and then scroll to the bottom to click \"Create\". You can ignore all the other options.\n",
        "8. Copy your token secret (\"hf_...\") and save it somewhere on your machine (e.g. Word or Notepad).\n",
        "9. Back in Colab, click on the key icon on the left hand side.\n",
        "10. Click on \"+ Add new secret\".\n",
        "11. Give the new secret the Name HF_TOKEN (please copy exactly this name).\n",
        "12. Paste in your token secret from step 8 as the Value.\n",
        "13. Make sure Notebook access is slide to the right. If done it will go blue and show a tick.\n",
        "15. Read on!"
      ],
      "metadata": {
        "id": "6O4GkckQH8BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the lecture we have seen the overall architecture and approach of transformers. We have also seen that in general transformers have allowed us to build very large-scale models of billions of paramaters (maybe even trillions).\n",
        "\n",
        "How should we interact with such models? It is obviously difficult for us to train our own transformer of this scale, and if we train a much smaller model, can it compete?\n",
        "\n",
        "Instead, we may want to build on top of a transfomer trained at web-scale. This Notebook will explore a couple of methods of doing this.\n",
        "\n",
        "First, let's install some packages and some data:"
      ],
      "metadata": {
        "id": "qYCPLrjSlzCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DISCLAIMER:\n",
        "I completed and uploaded this notebook to GitHub with all the outputs displayed, however, there was a rendering issue and the notebook wasn't being properly recognised in GitHub. I therefore have my answers to the various questions typed up, but this uploaded notebook must be run again to see all the corresponding outputs."
      ],
      "metadata": {
        "id": "pTKNJjJdza1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load Dataset and Tokeniser\n",
        "dataset = load_dataset(\"imdb\") # use the in-built IMDB review dataset"
      ],
      "metadata": {
        "id": "DkSL-vczc4eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used some standard libraries, most of which we have seen. We have also installed the IMDB dataset - a the name suggests a set of movie details and reviews.\n",
        "\n",
        "We will now start to use some transformer technology! We need to convert our data first of all to embeddings. We'll do so with the popular [BERT](https://arxiv.org/abs/1810.04805) model from Google:"
      ],
      "metadata": {
        "id": "INu0ErOsmuYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # we want the data to be the same size each time but some reviews are shorter\n",
        "    # if any are less than max_length, we will add zeros to the end of it until\n",
        "    # it is the same length as max_length (padding=\"max_length\")\n",
        "    # if any are longer than max_length (could happen in production), then\n",
        "    # we truncate it - i.e. delete any characters after max_length is reached.\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# tokenise the data (covert to embeddings)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "1ChUV7uNdAt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have tokenised our data using BERT ... i.e. converted the text documents to a vector format where each document is now a set of vectors that represents the semantic meaning of the text.\n",
        "\n",
        "We can use this feature extracted dataset in a variety of ways, two of which we will explore. The first is taking this as basically a feature engineered dataset, and training a simple prediction \"head\" on top of this. I.e. we will not attempt to train the transformer model at all - we will use it with the weights learned through the training regime Google used.\n",
        "\n",
        "Instead we will just use that as our backbone, with a simple, dense neural network on top that we will train. First, let's build that as a model:"
      ],
      "metadata": {
        "id": "SiQ1EBwtnYrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Embeddings-Only Model with Custom Head\n",
        "\n",
        "class CustomHeadModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_labels):\n",
        "        super(CustomHeadModel, self).__init__()\n",
        "        self.embedding_dim = embedding_dim # input\n",
        "        self.linear1 = nn.Linear(embedding_dim, 64) # linear layer\n",
        "        self.relu = nn.ReLU() # ReLU activation\n",
        "        self.linear2 = nn.Linear(64, num_labels) # linear layer - binary output\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        x = self.linear1(embeddings)\n",
        "        x = self.relu(x)\n",
        "        return self.linear2(x)\n",
        "\n",
        "# create a model from the pretrained embeddings\n",
        "model_embeddings = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# get the size of the embeddings (number of dimensions)\n",
        "embedding_dim = model_embeddings.config.hidden_size\n",
        "\n",
        "num_labels = 1 # Binary classification (positive: 0, negative: 1)\n",
        "\n",
        "# setup the custom head to take embeddings size as input and labels as output\n",
        "# note this time we are doing binary classification with two neurons to keep\n",
        "# num_labels as flexible and something we could change to a higher number of labels\n",
        "# this works the same way as output=1. Its slightly less efficient but fine\n",
        "custom_head = CustomHeadModel(embedding_dim, num_labels)"
      ],
      "metadata": {
        "id": "uGNE_vp-fakK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a similar codebase to the one we have seen before. The difference is essentialy at the input layer. Rather than feeding in raw data, we have fed in the embeddings from BERT.\n",
        "\n",
        "In our model the input layer is based on the shape of the embeddings we create with BERT and incorporate into our architecture via [AutoModel](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html). We then add a two layer DNN with ReLU and 64 neurons in the first layer and then we build our output layer as one neuron."
      ],
      "metadata": {
        "id": "LliGIjE0ol6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings-only prediction (with training of the classification head)\n",
        "\n",
        "# Freeze the embeddings\n",
        "for param in model_embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_embeddings.to(device) # add embedding model to GPU\n",
        "custom_head.to(device) # add custom head to GPU\n",
        "\n",
        "optimizer = optim.AdamW(custom_head.parameters(), lr=5e-5, weight_decay=0.0005) # Only optimise the custom head\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# use a subset of 1,000 random records for demonstration\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(1000))\n",
        "\n",
        "# Data is a list including:\n",
        "# 1. id of words\n",
        "# 2. attention mask\n",
        "# 3. label\n",
        "\n",
        "# Custom collate function to handle lists which DataLoader doesn't do automatically\n",
        "def custom_collate(batch):\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
        "    labels = torch.tensor([item['label'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': labels}\n",
        "\n",
        "\n",
        "train_loader = DataLoader(small_train_dataset, shuffle=True, batch_size=16, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "dtMB0dO8dNd-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the model we can set up for training. At the very start of the code, we have made the embeddings part of the model non-trainable (by setting these parameters as \"requires\\_grad = False\"). This means during training only the weights in the custom head will be updated in optimisation.\n",
        "\n",
        "We specify a loss function (BinaryCrossEntropy becuase we are using one neuron as output).\n",
        "\n",
        "We create a DataLoader as before. The only difference is we have a customer collate function because our embedding output will effectively be a list each time including id's, attention masks and labels. In the code above this, we load in a subset of the data (1,000 rows) to reduce training time."
      ],
      "metadata": {
        "id": "Z8_b6UfQqEQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "custom_head.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        epoch_loss = 0 # reset loss\n",
        "\n",
        "        # add data to GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        # Labels need to be float and unsqueezed for BCEWithLogitsLoss\n",
        "        labels = batch['label'].to(device).float().unsqueeze(1)\n",
        "\n",
        "        with torch.no_grad(): # Freeze the bert/embeddings model\n",
        "            embeddings = model_embeddings(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = custom_head(embeddings) # get logits for custom head\n",
        "        loss = criterion(logits, labels) # get loss for custom head\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {round(epoch_loss/len(train_loader), 4)}\")"
      ],
      "metadata": {
        "id": "1dD855X0dfgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fa0634-f0f6-43f7-f84e-ea48416d6954"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0104\n",
            "Epoch [2/10], Loss: 0.0111\n",
            "Epoch [3/10], Loss: 0.0105\n",
            "Epoch [4/10], Loss: 0.0105\n",
            "Epoch [5/10], Loss: 0.0105\n",
            "Epoch [6/10], Loss: 0.0103\n",
            "Epoch [7/10], Loss: 0.0102\n",
            "Epoch [8/10], Loss: 0.0101\n",
            "Epoch [9/10], Loss: 0.0102\n",
            "Epoch [10/10], Loss: 0.0105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training is a before. We can see from the very epoch we can see a low level of loss, and it stays about the same through each of the epochs. What does this tell you?\n",
        "\n",
        "#Answer:\n",
        "So we can see that the loss is very low from the get-go, the very first epoch has a loss of ~0.01. And the loss does not meaningfully decrease (coupled with the fact that predictions on trivially, simple examples are correct, seen below).\n",
        "\n",
        "So we can conclude that BERT embeddings are actually incredibly beneficial and informative to training the model, and the task is easy for a model that is pretrained using BERT. The custom head learns very quickly, converges, and the loss therefore stabilises.\n",
        "\n",
        "With a simple head, there is very little room for improvement.\n",
        "\n",
        "Now we can make predictions:"
      ],
      "metadata": {
        "id": "7YUTaKdfrbhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"This is a great movie!\"\n",
        "encoded_input = tokenizer(example_text, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    embeddings = model_embeddings(**encoded_input).pooler_output\n",
        "    logits = custom_head(embeddings)\n",
        "\n",
        "    # select the highest probability neuron as the prediction\n",
        "    predictions = torch.argmax(logits, dim=1) # dim 0 is neurons, dim 1 is probability of each\n",
        "\n",
        "    # print result\n",
        "    print_label = \"positive\" if predictions.item() == 0 else \"negative\"\n",
        "    print(f\"Prediction for '{example_text}': {print_label}\")"
      ],
      "metadata": {
        "id": "e6170GPMeuUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a287ed4-7139-48d2-fa97-901a51cf1a92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for 'This is a great movie!': positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We predict a very basic example \"This is a great movie!\". Unsuprisingly, given the low level of loss, this is easy for our model and it predicts it correctly.\n",
        "\n",
        "Now let's try another approach - fine-tuning. In this we will train the whole model:"
      ],
      "metadata": {
        "id": "gV2_DyTIrs6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning the Model with the Custom Head\n",
        "\n",
        "# Reset the custom head so it is untrained\n",
        "custom_head = CustomHeadModel(embedding_dim, num_labels)\n",
        "\n",
        "# combine the embeddings model and custom head into one model\n",
        "model_fine_tune = nn.Sequential(model_embeddings,custom_head)\n",
        "model_fine_tune.to(device) # add the combined model to GPU\n",
        "\n",
        "# this time optimise all parameters\n",
        "optimizer = optim.AdamW(model_fine_tune.parameters(), lr=5e-5, weight_decay=0.0005)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# use a subset of 1,000 records for demonstration\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle().select(range(1000))\n",
        "\n",
        "# Custom collate function to handle lists which DataLoader doesn't do automatically\n",
        "def custom_collate(batch):\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
        "    labels = torch.tensor([item['label'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': labels}\n",
        "\n",
        "train_loader = DataLoader(small_train_dataset, shuffle=True, batch_size=16, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "wYXqpjvvfCxY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have reset the model - reverted the weights of the custom head back to zero (actually random, but the same meaning).\n",
        "\n",
        "Now we build a new model, \"model\\_fine\\_tune\", with our embeddings and our reset, custom head. Note, we do not set any part of the model to \"frozen\" (i.e. \"requires\\_grad = False\") ... we will be training all of it.\n",
        "\n",
        "The last parts of the code just rebuild the DataLoader. If you are running the whole Notebook you don't actually need to do this, I've included it just in case you want to copy out for your own project.\n",
        "\n",
        "Now we can train as before:"
      ],
      "metadata": {
        "id": "M3J4V05lsB-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fine_tune.train()\n",
        "for epoch in range(num_epochs): #train for 3 epochs\n",
        "  for batch in train_loader:\n",
        "    epoch_loss = 0\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device).float().unsqueeze(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Pass input_ids and attention_mask to the first module in the sequence (model_embeddings)\n",
        "    # and then pass its output to the next module (custom_head)\n",
        "    outputs = model_fine_tune[1](model_fine_tune[0](input_ids=input_ids, attention_mask=attention_mask).pooler_output)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {round(epoch_loss/len(train_loader), 4)}\")"
      ],
      "metadata": {
        "id": "SU7JwktfiYzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee45152-f85d-4bf6-c897-a89c2e9fa128"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0109\n",
            "Epoch [2/10], Loss: 0.0113\n",
            "Epoch [3/10], Loss: 0.011\n",
            "Epoch [4/10], Loss: 0.0107\n",
            "Epoch [5/10], Loss: 0.0105\n",
            "Epoch [6/10], Loss: 0.0104\n",
            "Epoch [7/10], Loss: 0.0101\n",
            "Epoch [8/10], Loss: 0.0114\n",
            "Epoch [9/10], Loss: 0.0096\n",
            "Epoch [10/10], Loss: 0.0092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything here is code as we've previously seen. We also see the training outcomes which ... are about the same as before (although with a much slower training process as we are also training the embedding model). We can also try a prediction:"
      ],
      "metadata": {
        "id": "EN7soBjTtC_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"This is a terrible movie!\"\n",
        "encoded_input = tokenizer(example_text, return_tensors=\"pt\").to(device) # covert example to tokens\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Pass input_ids and attention_mask to the first module in the sequence (model_embeddings)\n",
        "    # and then pass its output to the next module (custom_head)\n",
        "    # **encoded_input means unpack the list of items\n",
        "    outputs = model_fine_tune[1](model_fine_tune[0](**encoded_input).pooler_output) # Pass encoded_input as keyword arguments\n",
        "\n",
        "    # select the highest probability neuron as the prediction\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # print result\n",
        "    print_label = \"positive\" if predictions.item() == 0 else \"negative\"\n",
        "    print(f\"Prediction for '{example_text}': {print_label}\")"
      ],
      "metadata": {
        "id": "qcRH_XIfimgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb4a3ea-d88a-4fcc-cdff-2c4dcf1e9777"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for 'This is a terrible movie!': positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a great prediction. How could we improve? What should we conclude about the two approaches?"
      ],
      "metadata": {
        "id": "GxM9D_YSuhrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answer:\n",
        "\n",
        "So now the training using a custom head, and all the BERT parameters is much more computationally expensive, and therefore slow. The loss outputs for each successive epoch are still around ~0.01, as earlier, but now the model's performance for a trivial prediction is off.\n",
        "\n",
        "There are several reasons as to why fine-tuning the model didn't improve its performance:\n",
        "\n",
        "1. The dataset is very small - it consists only of 1,000 samples. This small of a dataset means that gradients contain a lot of noise, so the model may be potentially overfit and doesn't perform well on data it doesn't know (as seen when asked about the prediction).\n",
        "\n",
        "2. The model was already efficient and optimum for the complexity of the task. The model earlier that was pretrained on BERT data was optimal, and fine-tuning only overfitted it.\n",
        "\n",
        "#Conclusion on the two approaches:\n",
        "\n",
        "Just using the embeddings (first approach) was much more faster, offered a stable, converging loss at successive epochs, a better performance, and didn't overfit the model. This was optimum for this dataset, as the dataset it not very expansive and was prone to being overfit following even a little fine-tuning.\n",
        "\n",
        "The second approach, using fine-tuning, was much more slower and computationally expensive. It clearly overfit, so the model performance deteriorated significantly, and it requires much more data than what the dataset offered to actually improve model performance.\n",
        "\n",
        "So using a frozen transformer as a feature extractor: much more efficient for limited data; fine-tuning is better for larger datasets/more complex tasks.\n"
      ],
      "metadata": {
        "id": "bIO9MrQiuEdA"
      }
    }
  ]
}