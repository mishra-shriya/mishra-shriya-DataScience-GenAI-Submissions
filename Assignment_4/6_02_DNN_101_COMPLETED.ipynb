{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mishra-shriya/mishra-shriya-DataScience-GenAI-Submissions/blob/main/Assignment_4/6_02_DNN_101_COMPLETED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "4a001889-eea3-4986-ab9a-908cc879104a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "9110578d-75b7-4e98-cfb6-fd3294077e05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "e42bd77f-f5e0-45ff-d3d1-8b7c074343ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 100 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "21d43e8c-69d6-4949-93ea-808990bc9bcd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 29396.4238\n",
            "Epoch [10/100], Loss: 31955.5566\n",
            "Epoch [10/100], Loss: 26065.0664\n",
            "Epoch [10/100], Loss: 30574.5664\n",
            "Epoch [10/100], Loss: 30843.7285\n",
            "Epoch [10/100], Loss: 28059.3691\n",
            "Epoch [10/100], Loss: 31405.4121\n",
            "Epoch [10/100], Loss: 24872.0938\n",
            "Epoch [20/100], Loss: 24022.0371\n",
            "Epoch [20/100], Loss: 24954.4102\n",
            "Epoch [20/100], Loss: 29392.666\n",
            "Epoch [20/100], Loss: 31409.2773\n",
            "Epoch [20/100], Loss: 29879.5\n",
            "Epoch [20/100], Loss: 36248.1992\n",
            "Epoch [20/100], Loss: 30985.4141\n",
            "Epoch [20/100], Loss: 39641.7188\n",
            "Epoch [30/100], Loss: 30831.9141\n",
            "Epoch [30/100], Loss: 29253.2734\n",
            "Epoch [30/100], Loss: 25690.6621\n",
            "Epoch [30/100], Loss: 27836.7363\n",
            "Epoch [30/100], Loss: 27686.2852\n",
            "Epoch [30/100], Loss: 31643.7344\n",
            "Epoch [30/100], Loss: 34332.3242\n",
            "Epoch [30/100], Loss: 19716.6445\n",
            "Epoch [40/100], Loss: 31755.0586\n",
            "Epoch [40/100], Loss: 26173.3691\n",
            "Epoch [40/100], Loss: 32958.1602\n",
            "Epoch [40/100], Loss: 35004.4023\n",
            "Epoch [40/100], Loss: 29000.2285\n",
            "Epoch [40/100], Loss: 27633.9102\n",
            "Epoch [40/100], Loss: 24313.1035\n",
            "Epoch [40/100], Loss: 5799.6885\n",
            "Epoch [50/100], Loss: 35374.0938\n",
            "Epoch [50/100], Loss: 25148.7891\n",
            "Epoch [50/100], Loss: 32456.5938\n",
            "Epoch [50/100], Loss: 24024.6387\n",
            "Epoch [50/100], Loss: 25031.3535\n",
            "Epoch [50/100], Loss: 32666.9727\n",
            "Epoch [50/100], Loss: 29857.3242\n",
            "Epoch [50/100], Loss: 11806.6719\n",
            "Epoch [60/100], Loss: 23889.8652\n",
            "Epoch [60/100], Loss: 26756.4648\n",
            "Epoch [60/100], Loss: 28301.2227\n",
            "Epoch [60/100], Loss: 27460.5742\n",
            "Epoch [60/100], Loss: 29907.0645\n",
            "Epoch [60/100], Loss: 28353.4512\n",
            "Epoch [60/100], Loss: 36269.3086\n",
            "Epoch [60/100], Loss: 23963.2402\n",
            "Epoch [70/100], Loss: 28261.4902\n",
            "Epoch [70/100], Loss: 27000.9453\n",
            "Epoch [70/100], Loss: 28135.9336\n",
            "Epoch [70/100], Loss: 26394.1211\n",
            "Epoch [70/100], Loss: 31803.8516\n",
            "Epoch [70/100], Loss: 26281.4102\n",
            "Epoch [70/100], Loss: 30083.1602\n",
            "Epoch [70/100], Loss: 6461.9316\n",
            "Epoch [80/100], Loss: 27982.0977\n",
            "Epoch [80/100], Loss: 27053.3984\n",
            "Epoch [80/100], Loss: 23368.2812\n",
            "Epoch [80/100], Loss: 28792.7578\n",
            "Epoch [80/100], Loss: 26150.9277\n",
            "Epoch [80/100], Loss: 25646.2793\n",
            "Epoch [80/100], Loss: 30807.7285\n",
            "Epoch [80/100], Loss: 54068.8672\n",
            "Epoch [90/100], Loss: 23620.4668\n",
            "Epoch [90/100], Loss: 19883.5664\n",
            "Epoch [90/100], Loss: 34424.9961\n",
            "Epoch [90/100], Loss: 30602.3848\n",
            "Epoch [90/100], Loss: 33430.6094\n",
            "Epoch [90/100], Loss: 20223.4219\n",
            "Epoch [90/100], Loss: 22428.5137\n",
            "Epoch [90/100], Loss: 26165.7246\n",
            "Epoch [100/100], Loss: 24630.7324\n",
            "Epoch [100/100], Loss: 30212.9668\n",
            "Epoch [100/100], Loss: 32318.291\n",
            "Epoch [100/100], Loss: 28344.8047\n",
            "Epoch [100/100], Loss: 20600.3984\n",
            "Epoch [100/100], Loss: 21579.5488\n",
            "Epoch [100/100], Loss: 17902.8301\n",
            "Epoch [100/100], Loss: 42189.6406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "d3c2bbd6-566a-4cef-a3ac-238af99606af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 21981.26171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "b7245cf6-abf5-4a49-8a1e-557be293cbec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Predicted  Actual\n",
              "0   16.107666   219.0\n",
              "1   15.067348    70.0\n",
              "2   15.952209   202.0\n",
              "3   19.069342   230.0\n",
              "4   15.249096   111.0\n",
              "..        ...     ...\n",
              "84  13.907419   153.0\n",
              "85  12.902101    98.0\n",
              "86  11.750359    37.0\n",
              "87  12.056334    63.0\n",
              "88  14.165344   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72d14b14-256f-4257-9cc2-c5f73b7d5163\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16.107666</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.067348</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15.952209</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19.069342</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15.249096</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>13.907419</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>12.902101</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>11.750359</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>12.056334</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>14.165344</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72d14b14-256f-4257-9cc2-c5f73b7d5163')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-72d14b14-256f-4257-9cc2-c5f73b7d5163 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-72d14b14-256f-4257-9cc2-c5f73b7d5163');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0424f4d0-a06e-4b5c-975e-0767fa1292a0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0424f4d0-a06e-4b5c-975e-0767fa1292a0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0424f4d0-a06e-4b5c-975e-0767fa1292a0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8e3e9178-736b-499d-8e06-9609fd854a33\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8e3e9178-736b-499d-8e06-9609fd854a33 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          15.71485424041748,\n          13.770841598510742,\n          15.561370849609375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?"
      ],
      "metadata": {
        "id": "rO6akT7a-sVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:\n",
        "To improve the side-by-side results, we could either:\n",
        "1. Increase the number of epochs and give the model greater training time.\n",
        "2. Normalise the target variable (as y is not scaled)."
      ],
      "metadata": {
        "id": "byOB1gDq-tab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise #1 Solution:\n",
        "\n",
        "So we can first try to increase the number of epochs to 1,000."
      ],
      "metadata": {
        "id": "ewz-f9LG62Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 1000 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "id": "xPRpk3ke7EQY",
        "outputId": "20c0d68a-4962-4c00-8320-54288a848839",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 22332.1914\n",
            "Epoch [10/1000], Loss: 23375.1738\n",
            "Epoch [10/1000], Loss: 22578.0703\n",
            "Epoch [10/1000], Loss: 26962.7852\n",
            "Epoch [10/1000], Loss: 24506.6602\n",
            "Epoch [10/1000], Loss: 23068.9062\n",
            "Epoch [10/1000], Loss: 25473.0898\n",
            "Epoch [10/1000], Loss: 10347.5029\n",
            "Epoch [20/1000], Loss: 22920.8438\n",
            "Epoch [20/1000], Loss: 21912.8438\n",
            "Epoch [20/1000], Loss: 26385.3398\n",
            "Epoch [20/1000], Loss: 25574.8379\n",
            "Epoch [20/1000], Loss: 21607.5488\n",
            "Epoch [20/1000], Loss: 16619.1094\n",
            "Epoch [20/1000], Loss: 22383.6387\n",
            "Epoch [20/1000], Loss: 18966.8047\n",
            "Epoch [30/1000], Loss: 29360.2734\n",
            "Epoch [30/1000], Loss: 20716.4316\n",
            "Epoch [30/1000], Loss: 16916.1992\n",
            "Epoch [30/1000], Loss: 16147.6025\n",
            "Epoch [30/1000], Loss: 18635.4434\n",
            "Epoch [30/1000], Loss: 20712.4121\n",
            "Epoch [30/1000], Loss: 23370.3203\n",
            "Epoch [30/1000], Loss: 22288.2383\n",
            "Epoch [40/1000], Loss: 23357.1445\n",
            "Epoch [40/1000], Loss: 16586.4688\n",
            "Epoch [40/1000], Loss: 20196.4551\n",
            "Epoch [40/1000], Loss: 20504.0371\n",
            "Epoch [40/1000], Loss: 17170.6836\n",
            "Epoch [40/1000], Loss: 19707.3262\n",
            "Epoch [40/1000], Loss: 16886.6953\n",
            "Epoch [40/1000], Loss: 11265.4648\n",
            "Epoch [50/1000], Loss: 21796.0527\n",
            "Epoch [50/1000], Loss: 16624.3652\n",
            "Epoch [50/1000], Loss: 19893.5117\n",
            "Epoch [50/1000], Loss: 11787.0947\n",
            "Epoch [50/1000], Loss: 16244.9521\n",
            "Epoch [50/1000], Loss: 20121.2949\n",
            "Epoch [50/1000], Loss: 15109.5771\n",
            "Epoch [50/1000], Loss: 21481.8203\n",
            "Epoch [60/1000], Loss: 18171.8926\n",
            "Epoch [60/1000], Loss: 17813.2969\n",
            "Epoch [60/1000], Loss: 16140.8223\n",
            "Epoch [60/1000], Loss: 15149.2744\n",
            "Epoch [60/1000], Loss: 13981.9307\n",
            "Epoch [60/1000], Loss: 13839.8574\n",
            "Epoch [60/1000], Loss: 15053.3662\n",
            "Epoch [60/1000], Loss: 4889.6777\n",
            "Epoch [70/1000], Loss: 12567.042\n",
            "Epoch [70/1000], Loss: 13359.5693\n",
            "Epoch [70/1000], Loss: 13903.79\n",
            "Epoch [70/1000], Loss: 16523.9941\n",
            "Epoch [70/1000], Loss: 13413.248\n",
            "Epoch [70/1000], Loss: 11751.5811\n",
            "Epoch [70/1000], Loss: 15072.3223\n",
            "Epoch [70/1000], Loss: 28822.4844\n",
            "Epoch [80/1000], Loss: 12945.2861\n",
            "Epoch [80/1000], Loss: 13371.0771\n",
            "Epoch [80/1000], Loss: 12775.9961\n",
            "Epoch [80/1000], Loss: 13374.8975\n",
            "Epoch [80/1000], Loss: 13346.6572\n",
            "Epoch [80/1000], Loss: 9147.29\n",
            "Epoch [80/1000], Loss: 11011.9082\n",
            "Epoch [80/1000], Loss: 11106.1484\n",
            "Epoch [90/1000], Loss: 10229.1221\n",
            "Epoch [90/1000], Loss: 11112.5352\n",
            "Epoch [90/1000], Loss: 9108.71\n",
            "Epoch [90/1000], Loss: 11643.7451\n",
            "Epoch [90/1000], Loss: 9501.5771\n",
            "Epoch [90/1000], Loss: 11533.0195\n",
            "Epoch [90/1000], Loss: 12225.7275\n",
            "Epoch [90/1000], Loss: 8454.8145\n",
            "Epoch [100/1000], Loss: 9386.7246\n",
            "Epoch [100/1000], Loss: 10611.5664\n",
            "Epoch [100/1000], Loss: 8072.8462\n",
            "Epoch [100/1000], Loss: 8831.0195\n",
            "Epoch [100/1000], Loss: 11071.1748\n",
            "Epoch [100/1000], Loss: 9275.9268\n",
            "Epoch [100/1000], Loss: 7926.8018\n",
            "Epoch [100/1000], Loss: 19314.498\n",
            "Epoch [110/1000], Loss: 7654.9321\n",
            "Epoch [110/1000], Loss: 9803.1641\n",
            "Epoch [110/1000], Loss: 10251.0234\n",
            "Epoch [110/1000], Loss: 8130.4985\n",
            "Epoch [110/1000], Loss: 7965.394\n",
            "Epoch [110/1000], Loss: 7556.4185\n",
            "Epoch [110/1000], Loss: 6437.4199\n",
            "Epoch [110/1000], Loss: 3043.3083\n",
            "Epoch [120/1000], Loss: 8526.9463\n",
            "Epoch [120/1000], Loss: 8073.8687\n",
            "Epoch [120/1000], Loss: 7180.8984\n",
            "Epoch [120/1000], Loss: 4972.4014\n",
            "Epoch [120/1000], Loss: 7922.0898\n",
            "Epoch [120/1000], Loss: 6586.709\n",
            "Epoch [120/1000], Loss: 6597.8682\n",
            "Epoch [120/1000], Loss: 15088.1641\n",
            "Epoch [130/1000], Loss: 6437.6499\n",
            "Epoch [130/1000], Loss: 5776.3135\n",
            "Epoch [130/1000], Loss: 5500.168\n",
            "Epoch [130/1000], Loss: 5848.6113\n",
            "Epoch [130/1000], Loss: 7590.791\n",
            "Epoch [130/1000], Loss: 7491.9312\n",
            "Epoch [130/1000], Loss: 6240.4775\n",
            "Epoch [130/1000], Loss: 2576.4656\n",
            "Epoch [140/1000], Loss: 4336.3042\n",
            "Epoch [140/1000], Loss: 8367.2119\n",
            "Epoch [140/1000], Loss: 5404.8198\n",
            "Epoch [140/1000], Loss: 5114.8882\n",
            "Epoch [140/1000], Loss: 5237.731\n",
            "Epoch [140/1000], Loss: 6838.5498\n",
            "Epoch [140/1000], Loss: 5262.1709\n",
            "Epoch [140/1000], Loss: 410.4407\n",
            "Epoch [150/1000], Loss: 5753.3047\n",
            "Epoch [150/1000], Loss: 4861.8682\n",
            "Epoch [150/1000], Loss: 5287.5938\n",
            "Epoch [150/1000], Loss: 5829.5137\n",
            "Epoch [150/1000], Loss: 4879.3711\n",
            "Epoch [150/1000], Loss: 4798.0713\n",
            "Epoch [150/1000], Loss: 5159.6689\n",
            "Epoch [150/1000], Loss: 9226.8594\n",
            "Epoch [160/1000], Loss: 3937.0452\n",
            "Epoch [160/1000], Loss: 5465.1436\n",
            "Epoch [160/1000], Loss: 4057.5933\n",
            "Epoch [160/1000], Loss: 5700.4424\n",
            "Epoch [160/1000], Loss: 5119.8506\n",
            "Epoch [160/1000], Loss: 5275.0112\n",
            "Epoch [160/1000], Loss: 4336.2793\n",
            "Epoch [160/1000], Loss: 7049.6113\n",
            "Epoch [170/1000], Loss: 3701.1445\n",
            "Epoch [170/1000], Loss: 5223.0645\n",
            "Epoch [170/1000], Loss: 4496.0142\n",
            "Epoch [170/1000], Loss: 5018.8291\n",
            "Epoch [170/1000], Loss: 5622.0674\n",
            "Epoch [170/1000], Loss: 4081.1765\n",
            "Epoch [170/1000], Loss: 3973.3486\n",
            "Epoch [170/1000], Loss: 6809.8174\n",
            "Epoch [180/1000], Loss: 3596.4124\n",
            "Epoch [180/1000], Loss: 6580.6748\n",
            "Epoch [180/1000], Loss: 4205.2212\n",
            "Epoch [180/1000], Loss: 3908.7571\n",
            "Epoch [180/1000], Loss: 3935.9131\n",
            "Epoch [180/1000], Loss: 3768.833\n",
            "Epoch [180/1000], Loss: 4826.1538\n",
            "Epoch [180/1000], Loss: 6875.9355\n",
            "Epoch [190/1000], Loss: 4328.3188\n",
            "Epoch [190/1000], Loss: 4319.6025\n",
            "Epoch [190/1000], Loss: 4026.8074\n",
            "Epoch [190/1000], Loss: 5243.3794\n",
            "Epoch [190/1000], Loss: 4819.0435\n",
            "Epoch [190/1000], Loss: 3508.3923\n",
            "Epoch [190/1000], Loss: 3910.739\n",
            "Epoch [190/1000], Loss: 2972.1489\n",
            "Epoch [200/1000], Loss: 4478.3232\n",
            "Epoch [200/1000], Loss: 3496.7869\n",
            "Epoch [200/1000], Loss: 5125.4351\n",
            "Epoch [200/1000], Loss: 2743.7793\n",
            "Epoch [200/1000], Loss: 6096.7524\n",
            "Epoch [200/1000], Loss: 3346.2424\n",
            "Epoch [200/1000], Loss: 4188.9531\n",
            "Epoch [200/1000], Loss: 5794.8896\n",
            "Epoch [210/1000], Loss: 4522.501\n",
            "Epoch [210/1000], Loss: 3716.3286\n",
            "Epoch [210/1000], Loss: 3149.6262\n",
            "Epoch [210/1000], Loss: 3988.1255\n",
            "Epoch [210/1000], Loss: 5265.0928\n",
            "Epoch [210/1000], Loss: 4306.5874\n",
            "Epoch [210/1000], Loss: 4198.3491\n",
            "Epoch [210/1000], Loss: 4256.4668\n",
            "Epoch [220/1000], Loss: 3482.6758\n",
            "Epoch [220/1000], Loss: 4806.397\n",
            "Epoch [220/1000], Loss: 3901.7568\n",
            "Epoch [220/1000], Loss: 3784.8408\n",
            "Epoch [220/1000], Loss: 4233.543\n",
            "Epoch [220/1000], Loss: 4918.3896\n",
            "Epoch [220/1000], Loss: 3602.3298\n",
            "Epoch [220/1000], Loss: 5394.2402\n",
            "Epoch [230/1000], Loss: 3437.4001\n",
            "Epoch [230/1000], Loss: 4114.2588\n",
            "Epoch [230/1000], Loss: 3714.2336\n",
            "Epoch [230/1000], Loss: 3502.655\n",
            "Epoch [230/1000], Loss: 4499.1719\n",
            "Epoch [230/1000], Loss: 5136.8389\n",
            "Epoch [230/1000], Loss: 4290.0532\n",
            "Epoch [230/1000], Loss: 1411.0736\n",
            "Epoch [240/1000], Loss: 3857.4043\n",
            "Epoch [240/1000], Loss: 4455.6904\n",
            "Epoch [240/1000], Loss: 5395.8799\n",
            "Epoch [240/1000], Loss: 4200.5576\n",
            "Epoch [240/1000], Loss: 2699.6721\n",
            "Epoch [240/1000], Loss: 3322.5471\n",
            "Epoch [240/1000], Loss: 4378.459\n",
            "Epoch [240/1000], Loss: 3623.2996\n",
            "Epoch [250/1000], Loss: 4472.2642\n",
            "Epoch [250/1000], Loss: 3937.3955\n",
            "Epoch [250/1000], Loss: 4675.8418\n",
            "Epoch [250/1000], Loss: 4204.5215\n",
            "Epoch [250/1000], Loss: 4081.3098\n",
            "Epoch [250/1000], Loss: 3721.5574\n",
            "Epoch [250/1000], Loss: 3112.8906\n",
            "Epoch [250/1000], Loss: 1678.7728\n",
            "Epoch [260/1000], Loss: 3575.4636\n",
            "Epoch [260/1000], Loss: 4430.7202\n",
            "Epoch [260/1000], Loss: 2983.373\n",
            "Epoch [260/1000], Loss: 4618.3916\n",
            "Epoch [260/1000], Loss: 3206.4302\n",
            "Epoch [260/1000], Loss: 4588.0952\n",
            "Epoch [260/1000], Loss: 4586.0332\n",
            "Epoch [260/1000], Loss: 1740.0713\n",
            "Epoch [270/1000], Loss: 4461.416\n",
            "Epoch [270/1000], Loss: 3562.5869\n",
            "Epoch [270/1000], Loss: 3574.3694\n",
            "Epoch [270/1000], Loss: 4247.7676\n",
            "Epoch [270/1000], Loss: 4290.2285\n",
            "Epoch [270/1000], Loss: 3728.3425\n",
            "Epoch [270/1000], Loss: 3673.9299\n",
            "Epoch [270/1000], Loss: 5665.7256\n",
            "Epoch [280/1000], Loss: 3695.4827\n",
            "Epoch [280/1000], Loss: 3670.5286\n",
            "Epoch [280/1000], Loss: 4228.0776\n",
            "Epoch [280/1000], Loss: 3282.8901\n",
            "Epoch [280/1000], Loss: 3962.5103\n",
            "Epoch [280/1000], Loss: 4562.522\n",
            "Epoch [280/1000], Loss: 3889.99\n",
            "Epoch [280/1000], Loss: 6700.5581\n",
            "Epoch [290/1000], Loss: 4654.395\n",
            "Epoch [290/1000], Loss: 3791.0808\n",
            "Epoch [290/1000], Loss: 3683.9409\n",
            "Epoch [290/1000], Loss: 3350.6936\n",
            "Epoch [290/1000], Loss: 3888.9421\n",
            "Epoch [290/1000], Loss: 4750.1797\n",
            "Epoch [290/1000], Loss: 3035.8062\n",
            "Epoch [290/1000], Loss: 5585.3687\n",
            "Epoch [300/1000], Loss: 4613.5332\n",
            "Epoch [300/1000], Loss: 3378.5505\n",
            "Epoch [300/1000], Loss: 4506.1758\n",
            "Epoch [300/1000], Loss: 3564.4675\n",
            "Epoch [300/1000], Loss: 2133.4175\n",
            "Epoch [300/1000], Loss: 3881.4993\n",
            "Epoch [300/1000], Loss: 4604.6499\n",
            "Epoch [300/1000], Loss: 10312.375\n",
            "Epoch [310/1000], Loss: 3676.6387\n",
            "Epoch [310/1000], Loss: 3847.1279\n",
            "Epoch [310/1000], Loss: 3475.23\n",
            "Epoch [310/1000], Loss: 3368.4258\n",
            "Epoch [310/1000], Loss: 4483.9766\n",
            "Epoch [310/1000], Loss: 4676.5518\n",
            "Epoch [310/1000], Loss: 3188.918\n",
            "Epoch [310/1000], Loss: 6744.6929\n",
            "Epoch [320/1000], Loss: 4177.6313\n",
            "Epoch [320/1000], Loss: 3862.907\n",
            "Epoch [320/1000], Loss: 3945.2236\n",
            "Epoch [320/1000], Loss: 3418.9377\n",
            "Epoch [320/1000], Loss: 3766.4031\n",
            "Epoch [320/1000], Loss: 3550.1465\n",
            "Epoch [320/1000], Loss: 4079.6099\n",
            "Epoch [320/1000], Loss: 2207.2822\n",
            "Epoch [330/1000], Loss: 4608.4805\n",
            "Epoch [330/1000], Loss: 3903.249\n",
            "Epoch [330/1000], Loss: 3099.2905\n",
            "Epoch [330/1000], Loss: 3434.5896\n",
            "Epoch [330/1000], Loss: 4874.0356\n",
            "Epoch [330/1000], Loss: 3729.3506\n",
            "Epoch [330/1000], Loss: 2900.9136\n",
            "Epoch [330/1000], Loss: 3474.4609\n",
            "Epoch [340/1000], Loss: 4329.9097\n",
            "Epoch [340/1000], Loss: 3047.9043\n",
            "Epoch [340/1000], Loss: 3721.6455\n",
            "Epoch [340/1000], Loss: 4847.6484\n",
            "Epoch [340/1000], Loss: 3978.0417\n",
            "Epoch [340/1000], Loss: 2975.0754\n",
            "Epoch [340/1000], Loss: 3640.0083\n",
            "Epoch [340/1000], Loss: 901.9778\n",
            "Epoch [350/1000], Loss: 4080.125\n",
            "Epoch [350/1000], Loss: 3904.7202\n",
            "Epoch [350/1000], Loss: 3668.7791\n",
            "Epoch [350/1000], Loss: 4191.5747\n",
            "Epoch [350/1000], Loss: 2487.5784\n",
            "Epoch [350/1000], Loss: 3638.8193\n",
            "Epoch [350/1000], Loss: 4226.6538\n",
            "Epoch [350/1000], Loss: 4070.4829\n",
            "Epoch [360/1000], Loss: 3522.6975\n",
            "Epoch [360/1000], Loss: 4309.917\n",
            "Epoch [360/1000], Loss: 3379.3699\n",
            "Epoch [360/1000], Loss: 2659.5371\n",
            "Epoch [360/1000], Loss: 4214.5029\n",
            "Epoch [360/1000], Loss: 2929.7212\n",
            "Epoch [360/1000], Loss: 5024.2178\n",
            "Epoch [360/1000], Loss: 3668.2205\n",
            "Epoch [370/1000], Loss: 4571.0239\n",
            "Epoch [370/1000], Loss: 3030.1394\n",
            "Epoch [370/1000], Loss: 3779.2332\n",
            "Epoch [370/1000], Loss: 3708.8481\n",
            "Epoch [370/1000], Loss: 3668.8213\n",
            "Epoch [370/1000], Loss: 3071.1775\n",
            "Epoch [370/1000], Loss: 4035.4783\n",
            "Epoch [370/1000], Loss: 4012.3396\n",
            "Epoch [380/1000], Loss: 3089.0793\n",
            "Epoch [380/1000], Loss: 4228.4888\n",
            "Epoch [380/1000], Loss: 3636.5881\n",
            "Epoch [380/1000], Loss: 3857.0049\n",
            "Epoch [380/1000], Loss: 2255.877\n",
            "Epoch [380/1000], Loss: 5137.2793\n",
            "Epoch [380/1000], Loss: 3440.179\n",
            "Epoch [380/1000], Loss: 5365.6201\n",
            "Epoch [390/1000], Loss: 4402.2266\n",
            "Epoch [390/1000], Loss: 3318.8281\n",
            "Epoch [390/1000], Loss: 3188.8755\n",
            "Epoch [390/1000], Loss: 3624.52\n",
            "Epoch [390/1000], Loss: 4714.2793\n",
            "Epoch [390/1000], Loss: 2743.0718\n",
            "Epoch [390/1000], Loss: 3567.2549\n",
            "Epoch [390/1000], Loss: 4161.7021\n",
            "Epoch [400/1000], Loss: 3662.6138\n",
            "Epoch [400/1000], Loss: 3124.0798\n",
            "Epoch [400/1000], Loss: 3663.2842\n",
            "Epoch [400/1000], Loss: 4269.1025\n",
            "Epoch [400/1000], Loss: 3896.0854\n",
            "Epoch [400/1000], Loss: 3607.1643\n",
            "Epoch [400/1000], Loss: 3339.228\n",
            "Epoch [400/1000], Loss: 1544.514\n",
            "Epoch [410/1000], Loss: 4586.0034\n",
            "Epoch [410/1000], Loss: 2719.6296\n",
            "Epoch [410/1000], Loss: 4086.7366\n",
            "Epoch [410/1000], Loss: 4309.7632\n",
            "Epoch [410/1000], Loss: 3299.3674\n",
            "Epoch [410/1000], Loss: 3306.1233\n",
            "Epoch [410/1000], Loss: 3147.1653\n",
            "Epoch [410/1000], Loss: 948.6265\n",
            "Epoch [420/1000], Loss: 4682.1982\n",
            "Epoch [420/1000], Loss: 3828.5234\n",
            "Epoch [420/1000], Loss: 3778.9062\n",
            "Epoch [420/1000], Loss: 2945.9087\n",
            "Epoch [420/1000], Loss: 3554.6636\n",
            "Epoch [420/1000], Loss: 2728.4053\n",
            "Epoch [420/1000], Loss: 3548.5137\n",
            "Epoch [420/1000], Loss: 5121.2144\n",
            "Epoch [430/1000], Loss: 4494.603\n",
            "Epoch [430/1000], Loss: 3497.3667\n",
            "Epoch [430/1000], Loss: 3708.6206\n",
            "Epoch [430/1000], Loss: 3960.4141\n",
            "Epoch [430/1000], Loss: 2505.1685\n",
            "Epoch [430/1000], Loss: 3728.2388\n",
            "Epoch [430/1000], Loss: 3152.9492\n",
            "Epoch [430/1000], Loss: 3353.0928\n",
            "Epoch [440/1000], Loss: 3484.269\n",
            "Epoch [440/1000], Loss: 3902.9749\n",
            "Epoch [440/1000], Loss: 3240.8884\n",
            "Epoch [440/1000], Loss: 3481.4448\n",
            "Epoch [440/1000], Loss: 3767.793\n",
            "Epoch [440/1000], Loss: 3287.6968\n",
            "Epoch [440/1000], Loss: 3749.0215\n",
            "Epoch [440/1000], Loss: 3234.4775\n",
            "Epoch [450/1000], Loss: 3467.7068\n",
            "Epoch [450/1000], Loss: 3023.7097\n",
            "Epoch [450/1000], Loss: 2919.6306\n",
            "Epoch [450/1000], Loss: 4755.791\n",
            "Epoch [450/1000], Loss: 3061.7166\n",
            "Epoch [450/1000], Loss: 3706.0833\n",
            "Epoch [450/1000], Loss: 3734.1272\n",
            "Epoch [450/1000], Loss: 4984.105\n",
            "Epoch [460/1000], Loss: 4346.1426\n",
            "Epoch [460/1000], Loss: 3506.9905\n",
            "Epoch [460/1000], Loss: 3227.8081\n",
            "Epoch [460/1000], Loss: 3171.105\n",
            "Epoch [460/1000], Loss: 3792.3093\n",
            "Epoch [460/1000], Loss: 3305.4683\n",
            "Epoch [460/1000], Loss: 3407.0486\n",
            "Epoch [460/1000], Loss: 1232.433\n",
            "Epoch [470/1000], Loss: 3167.8943\n",
            "Epoch [470/1000], Loss: 3115.4263\n",
            "Epoch [470/1000], Loss: 3224.7065\n",
            "Epoch [470/1000], Loss: 3362.8159\n",
            "Epoch [470/1000], Loss: 3365.7593\n",
            "Epoch [470/1000], Loss: 4285.6226\n",
            "Epoch [470/1000], Loss: 3889.3818\n",
            "Epoch [470/1000], Loss: 4909.3696\n",
            "Epoch [480/1000], Loss: 4257.3662\n",
            "Epoch [480/1000], Loss: 3144.1138\n",
            "Epoch [480/1000], Loss: 3108.574\n",
            "Epoch [480/1000], Loss: 2944.4988\n",
            "Epoch [480/1000], Loss: 3566.0498\n",
            "Epoch [480/1000], Loss: 3799.3118\n",
            "Epoch [480/1000], Loss: 3502.3743\n",
            "Epoch [480/1000], Loss: 4209.1416\n",
            "Epoch [490/1000], Loss: 3537.9067\n",
            "Epoch [490/1000], Loss: 4104.6338\n",
            "Epoch [490/1000], Loss: 3467.0947\n",
            "Epoch [490/1000], Loss: 3310.2773\n",
            "Epoch [490/1000], Loss: 3243.8213\n",
            "Epoch [490/1000], Loss: 3298.6567\n",
            "Epoch [490/1000], Loss: 3224.9597\n",
            "Epoch [490/1000], Loss: 4705.4331\n",
            "Epoch [500/1000], Loss: 3373.9021\n",
            "Epoch [500/1000], Loss: 3987.2373\n",
            "Epoch [500/1000], Loss: 2619.9485\n",
            "Epoch [500/1000], Loss: 4110.7461\n",
            "Epoch [500/1000], Loss: 2797.6948\n",
            "Epoch [500/1000], Loss: 4507.2129\n",
            "Epoch [500/1000], Loss: 2547.5615\n",
            "Epoch [500/1000], Loss: 6890.6025\n",
            "Epoch [510/1000], Loss: 2696.9016\n",
            "Epoch [510/1000], Loss: 4939.1738\n",
            "Epoch [510/1000], Loss: 3382.2236\n",
            "Epoch [510/1000], Loss: 2864.2561\n",
            "Epoch [510/1000], Loss: 2902.8894\n",
            "Epoch [510/1000], Loss: 3720.7761\n",
            "Epoch [510/1000], Loss: 3638.7468\n",
            "Epoch [510/1000], Loss: 1618.8972\n",
            "Epoch [520/1000], Loss: 4381.7017\n",
            "Epoch [520/1000], Loss: 3518.01\n",
            "Epoch [520/1000], Loss: 3107.6372\n",
            "Epoch [520/1000], Loss: 3554.5173\n",
            "Epoch [520/1000], Loss: 2832.865\n",
            "Epoch [520/1000], Loss: 3562.7505\n",
            "Epoch [520/1000], Loss: 3111.6836\n",
            "Epoch [520/1000], Loss: 1190.2897\n",
            "Epoch [530/1000], Loss: 2857.207\n",
            "Epoch [530/1000], Loss: 4903.2163\n",
            "Epoch [530/1000], Loss: 3508.6025\n",
            "Epoch [530/1000], Loss: 3792.6162\n",
            "Epoch [530/1000], Loss: 3117.5188\n",
            "Epoch [530/1000], Loss: 2886.3274\n",
            "Epoch [530/1000], Loss: 2845.741\n",
            "Epoch [530/1000], Loss: 2343.729\n",
            "Epoch [540/1000], Loss: 2642.5669\n",
            "Epoch [540/1000], Loss: 3268.4363\n",
            "Epoch [540/1000], Loss: 3190.2322\n",
            "Epoch [540/1000], Loss: 3561.6965\n",
            "Epoch [540/1000], Loss: 4396.8081\n",
            "Epoch [540/1000], Loss: 3171.9749\n",
            "Epoch [540/1000], Loss: 3665.1331\n",
            "Epoch [540/1000], Loss: 674.3507\n",
            "Epoch [550/1000], Loss: 3499.6279\n",
            "Epoch [550/1000], Loss: 3192.6543\n",
            "Epoch [550/1000], Loss: 3574.5752\n",
            "Epoch [550/1000], Loss: 3609.8271\n",
            "Epoch [550/1000], Loss: 2913.8696\n",
            "Epoch [550/1000], Loss: 3582.1655\n",
            "Epoch [550/1000], Loss: 3396.2627\n",
            "Epoch [550/1000], Loss: 1002.5683\n",
            "Epoch [560/1000], Loss: 3404.1641\n",
            "Epoch [560/1000], Loss: 4313.188\n",
            "Epoch [560/1000], Loss: 3368.0598\n",
            "Epoch [560/1000], Loss: 3000.8574\n",
            "Epoch [560/1000], Loss: 3082.9099\n",
            "Epoch [560/1000], Loss: 2731.0713\n",
            "Epoch [560/1000], Loss: 3514.4099\n",
            "Epoch [560/1000], Loss: 5352.0874\n",
            "Epoch [570/1000], Loss: 2840.8069\n",
            "Epoch [570/1000], Loss: 3534.1479\n",
            "Epoch [570/1000], Loss: 2951.8923\n",
            "Epoch [570/1000], Loss: 4068.739\n",
            "Epoch [570/1000], Loss: 2972.9348\n",
            "Epoch [570/1000], Loss: 3816.6299\n",
            "Epoch [570/1000], Loss: 3128.7261\n",
            "Epoch [570/1000], Loss: 5433.3965\n",
            "Epoch [580/1000], Loss: 4003.3516\n",
            "Epoch [580/1000], Loss: 3849.5674\n",
            "Epoch [580/1000], Loss: 2717.5073\n",
            "Epoch [580/1000], Loss: 3274.6753\n",
            "Epoch [580/1000], Loss: 3451.1528\n",
            "Epoch [580/1000], Loss: 3377.0486\n",
            "Epoch [580/1000], Loss: 2705.1318\n",
            "Epoch [580/1000], Loss: 2735.6299\n",
            "Epoch [590/1000], Loss: 4003.092\n",
            "Epoch [590/1000], Loss: 2723.6543\n",
            "Epoch [590/1000], Loss: 2890.4644\n",
            "Epoch [590/1000], Loss: 4038.1316\n",
            "Epoch [590/1000], Loss: 3582.9021\n",
            "Epoch [590/1000], Loss: 3350.8884\n",
            "Epoch [590/1000], Loss: 2585.9214\n",
            "Epoch [590/1000], Loss: 4841.314\n",
            "Epoch [600/1000], Loss: 4006.7288\n",
            "Epoch [600/1000], Loss: 3664.7786\n",
            "Epoch [600/1000], Loss: 3235.3818\n",
            "Epoch [600/1000], Loss: 3448.5815\n",
            "Epoch [600/1000], Loss: 3201.0291\n",
            "Epoch [600/1000], Loss: 2742.5508\n",
            "Epoch [600/1000], Loss: 3016.3105\n",
            "Epoch [600/1000], Loss: 930.5077\n",
            "Epoch [610/1000], Loss: 3481.4365\n",
            "Epoch [610/1000], Loss: 2864.2502\n",
            "Epoch [610/1000], Loss: 2720.6025\n",
            "Epoch [610/1000], Loss: 3796.2729\n",
            "Epoch [610/1000], Loss: 3258.6011\n",
            "Epoch [610/1000], Loss: 4164.4375\n",
            "Epoch [610/1000], Loss: 2904.9387\n",
            "Epoch [610/1000], Loss: 1568.9475\n",
            "Epoch [620/1000], Loss: 3740.0027\n",
            "Epoch [620/1000], Loss: 3421.7236\n",
            "Epoch [620/1000], Loss: 2639.9155\n",
            "Epoch [620/1000], Loss: 2849.7986\n",
            "Epoch [620/1000], Loss: 3139.26\n",
            "Epoch [620/1000], Loss: 4516.2529\n",
            "Epoch [620/1000], Loss: 2814.4631\n",
            "Epoch [620/1000], Loss: 1422.3218\n",
            "Epoch [630/1000], Loss: 3540.9753\n",
            "Epoch [630/1000], Loss: 3393.8867\n",
            "Epoch [630/1000], Loss: 3769.9644\n",
            "Epoch [630/1000], Loss: 3382.8262\n",
            "Epoch [630/1000], Loss: 3986.4355\n",
            "Epoch [630/1000], Loss: 2450.5249\n",
            "Epoch [630/1000], Loss: 2221.5022\n",
            "Epoch [630/1000], Loss: 6402.3701\n",
            "Epoch [640/1000], Loss: 3337.2053\n",
            "Epoch [640/1000], Loss: 3150.334\n",
            "Epoch [640/1000], Loss: 3243.1431\n",
            "Epoch [640/1000], Loss: 3636.9243\n",
            "Epoch [640/1000], Loss: 1709.5349\n",
            "Epoch [640/1000], Loss: 3724.9626\n",
            "Epoch [640/1000], Loss: 4143.2056\n",
            "Epoch [640/1000], Loss: 1800.7136\n",
            "Epoch [650/1000], Loss: 2677.1868\n",
            "Epoch [650/1000], Loss: 2579.6951\n",
            "Epoch [650/1000], Loss: 2740.4209\n",
            "Epoch [650/1000], Loss: 2802.728\n",
            "Epoch [650/1000], Loss: 3765.0906\n",
            "Epoch [650/1000], Loss: 4665.9556\n",
            "Epoch [650/1000], Loss: 3609.0247\n",
            "Epoch [650/1000], Loss: 2272.1697\n",
            "Epoch [660/1000], Loss: 3205.7361\n",
            "Epoch [660/1000], Loss: 3187.115\n",
            "Epoch [660/1000], Loss: 3914.4312\n",
            "Epoch [660/1000], Loss: 4040.417\n",
            "Epoch [660/1000], Loss: 2497.575\n",
            "Epoch [660/1000], Loss: 2772.2319\n",
            "Epoch [660/1000], Loss: 3021.8782\n",
            "Epoch [660/1000], Loss: 4494.9453\n",
            "Epoch [670/1000], Loss: 3413.438\n",
            "Epoch [670/1000], Loss: 3838.0037\n",
            "Epoch [670/1000], Loss: 2667.2849\n",
            "Epoch [670/1000], Loss: 2409.5376\n",
            "Epoch [670/1000], Loss: 3825.7805\n",
            "Epoch [670/1000], Loss: 2961.2625\n",
            "Epoch [670/1000], Loss: 3473.5012\n",
            "Epoch [670/1000], Loss: 4036.3992\n",
            "Epoch [680/1000], Loss: 3270.4536\n",
            "Epoch [680/1000], Loss: 3017.9792\n",
            "Epoch [680/1000], Loss: 3780.02\n",
            "Epoch [680/1000], Loss: 3430.6094\n",
            "Epoch [680/1000], Loss: 2179.5679\n",
            "Epoch [680/1000], Loss: 3862.8862\n",
            "Epoch [680/1000], Loss: 2766.1104\n",
            "Epoch [680/1000], Loss: 7675.1401\n",
            "Epoch [690/1000], Loss: 3591.4502\n",
            "Epoch [690/1000], Loss: 2580.5029\n",
            "Epoch [690/1000], Loss: 3968.3667\n",
            "Epoch [690/1000], Loss: 2891.4932\n",
            "Epoch [690/1000], Loss: 3096.4399\n",
            "Epoch [690/1000], Loss: 3204.9866\n",
            "Epoch [690/1000], Loss: 3288.4849\n",
            "Epoch [690/1000], Loss: 1268.8845\n",
            "Epoch [700/1000], Loss: 2886.3264\n",
            "Epoch [700/1000], Loss: 3286.1565\n",
            "Epoch [700/1000], Loss: 2656.8604\n",
            "Epoch [700/1000], Loss: 3089.7488\n",
            "Epoch [700/1000], Loss: 3116.177\n",
            "Epoch [700/1000], Loss: 3980.0425\n",
            "Epoch [700/1000], Loss: 3364.1028\n",
            "Epoch [700/1000], Loss: 4333.9883\n",
            "Epoch [710/1000], Loss: 3374.8281\n",
            "Epoch [710/1000], Loss: 2300.4399\n",
            "Epoch [710/1000], Loss: 3076.2515\n",
            "Epoch [710/1000], Loss: 3602.5708\n",
            "Epoch [710/1000], Loss: 3068.3142\n",
            "Epoch [710/1000], Loss: 3212.6655\n",
            "Epoch [710/1000], Loss: 3869.5393\n",
            "Epoch [710/1000], Loss: 1099.9209\n",
            "Epoch [720/1000], Loss: 3034.4197\n",
            "Epoch [720/1000], Loss: 3814.2715\n",
            "Epoch [720/1000], Loss: 2506.9285\n",
            "Epoch [720/1000], Loss: 4347.98\n",
            "Epoch [720/1000], Loss: 2568.2314\n",
            "Epoch [720/1000], Loss: 3363.8887\n",
            "Epoch [720/1000], Loss: 2578.0942\n",
            "Epoch [720/1000], Loss: 4934.4219\n",
            "Epoch [730/1000], Loss: 2303.8586\n",
            "Epoch [730/1000], Loss: 2886.6484\n",
            "Epoch [730/1000], Loss: 3735.5464\n",
            "Epoch [730/1000], Loss: 2950.4543\n",
            "Epoch [730/1000], Loss: 3817.0886\n",
            "Epoch [730/1000], Loss: 3418.3262\n",
            "Epoch [730/1000], Loss: 3320.9197\n",
            "Epoch [730/1000], Loss: 224.4415\n",
            "Epoch [740/1000], Loss: 2903.8958\n",
            "Epoch [740/1000], Loss: 2171.7537\n",
            "Epoch [740/1000], Loss: 3326.7786\n",
            "Epoch [740/1000], Loss: 3877.0425\n",
            "Epoch [740/1000], Loss: 3434.7305\n",
            "Epoch [740/1000], Loss: 3466.218\n",
            "Epoch [740/1000], Loss: 2996.6724\n",
            "Epoch [740/1000], Loss: 3569.6943\n",
            "Epoch [750/1000], Loss: 3362.6167\n",
            "Epoch [750/1000], Loss: 3879.5312\n",
            "Epoch [750/1000], Loss: 3609.5503\n",
            "Epoch [750/1000], Loss: 2755.4802\n",
            "Epoch [750/1000], Loss: 2840.8169\n",
            "Epoch [750/1000], Loss: 2967.8818\n",
            "Epoch [750/1000], Loss: 2742.459\n",
            "Epoch [750/1000], Loss: 2848.0605\n",
            "Epoch [760/1000], Loss: 3238.3594\n",
            "Epoch [760/1000], Loss: 2649.6675\n",
            "Epoch [760/1000], Loss: 2624.8979\n",
            "Epoch [760/1000], Loss: 3223.6787\n",
            "Epoch [760/1000], Loss: 3254.0637\n",
            "Epoch [760/1000], Loss: 3649.5225\n",
            "Epoch [760/1000], Loss: 3457.8049\n",
            "Epoch [760/1000], Loss: 2839.3289\n",
            "Epoch [770/1000], Loss: 3169.4871\n",
            "Epoch [770/1000], Loss: 3870.0659\n",
            "Epoch [770/1000], Loss: 3807.1423\n",
            "Epoch [770/1000], Loss: 3090.9062\n",
            "Epoch [770/1000], Loss: 3792.238\n",
            "Epoch [770/1000], Loss: 2251.731\n",
            "Epoch [770/1000], Loss: 2102.6641\n",
            "Epoch [770/1000], Loss: 2189.6836\n",
            "Epoch [780/1000], Loss: 2850.1438\n",
            "Epoch [780/1000], Loss: 3136.8079\n",
            "Epoch [780/1000], Loss: 3191.5728\n",
            "Epoch [780/1000], Loss: 4255.5234\n",
            "Epoch [780/1000], Loss: 2480.2327\n",
            "Epoch [780/1000], Loss: 3480.4619\n",
            "Epoch [780/1000], Loss: 2717.9961\n",
            "Epoch [780/1000], Loss: 981.4336\n",
            "Epoch [790/1000], Loss: 3555.7908\n",
            "Epoch [790/1000], Loss: 3533.7502\n",
            "Epoch [790/1000], Loss: 3452.7673\n",
            "Epoch [790/1000], Loss: 2788.7822\n",
            "Epoch [790/1000], Loss: 3757.1724\n",
            "Epoch [790/1000], Loss: 1968.0659\n",
            "Epoch [790/1000], Loss: 2924.2134\n",
            "Epoch [790/1000], Loss: 2302.7371\n",
            "Epoch [800/1000], Loss: 2452.2783\n",
            "Epoch [800/1000], Loss: 3263.9412\n",
            "Epoch [800/1000], Loss: 2890.9141\n",
            "Epoch [800/1000], Loss: 2848.3069\n",
            "Epoch [800/1000], Loss: 3562.1653\n",
            "Epoch [800/1000], Loss: 3890.9475\n",
            "Epoch [800/1000], Loss: 3007.6809\n",
            "Epoch [800/1000], Loss: 2465.7739\n",
            "Epoch [810/1000], Loss: 2434.2158\n",
            "Epoch [810/1000], Loss: 2850.8499\n",
            "Epoch [810/1000], Loss: 3575.5459\n",
            "Epoch [810/1000], Loss: 3557.0881\n",
            "Epoch [810/1000], Loss: 2742.168\n",
            "Epoch [810/1000], Loss: 2991.4944\n",
            "Epoch [810/1000], Loss: 3611.1641\n",
            "Epoch [810/1000], Loss: 4158.96\n",
            "Epoch [820/1000], Loss: 3596.7561\n",
            "Epoch [820/1000], Loss: 2535.217\n",
            "Epoch [820/1000], Loss: 3637.5178\n",
            "Epoch [820/1000], Loss: 3075.1541\n",
            "Epoch [820/1000], Loss: 2312.106\n",
            "Epoch [820/1000], Loss: 3719.1335\n",
            "Epoch [820/1000], Loss: 2635.0503\n",
            "Epoch [820/1000], Loss: 7491.54\n",
            "Epoch [830/1000], Loss: 2723.054\n",
            "Epoch [830/1000], Loss: 3077.6333\n",
            "Epoch [830/1000], Loss: 3338.97\n",
            "Epoch [830/1000], Loss: 3927.052\n",
            "Epoch [830/1000], Loss: 2970.3042\n",
            "Epoch [830/1000], Loss: 2475.2539\n",
            "Epoch [830/1000], Loss: 3061.4778\n",
            "Epoch [830/1000], Loss: 5659.5718\n",
            "Epoch [840/1000], Loss: 2949.7288\n",
            "Epoch [840/1000], Loss: 2736.543\n",
            "Epoch [840/1000], Loss: 2827.0081\n",
            "Epoch [840/1000], Loss: 3033.4119\n",
            "Epoch [840/1000], Loss: 3652.4917\n",
            "Epoch [840/1000], Loss: 3746.6748\n",
            "Epoch [840/1000], Loss: 2582.3918\n",
            "Epoch [840/1000], Loss: 5651.1963\n",
            "Epoch [850/1000], Loss: 2636.1252\n",
            "Epoch [850/1000], Loss: 2972.97\n",
            "Epoch [850/1000], Loss: 2772.3125\n",
            "Epoch [850/1000], Loss: 3906.4055\n",
            "Epoch [850/1000], Loss: 3114.9197\n",
            "Epoch [850/1000], Loss: 3331.4465\n",
            "Epoch [850/1000], Loss: 3029.0933\n",
            "Epoch [850/1000], Loss: 933.7186\n",
            "Epoch [860/1000], Loss: 2959.8596\n",
            "Epoch [860/1000], Loss: 2853.5227\n",
            "Epoch [860/1000], Loss: 3783.8708\n",
            "Epoch [860/1000], Loss: 3502.3262\n",
            "Epoch [860/1000], Loss: 2457.72\n",
            "Epoch [860/1000], Loss: 3366.8481\n",
            "Epoch [860/1000], Loss: 2820.0686\n",
            "Epoch [860/1000], Loss: 541.4344\n",
            "Epoch [870/1000], Loss: 4344.7949\n",
            "Epoch [870/1000], Loss: 3002.3867\n",
            "Epoch [870/1000], Loss: 2732.1897\n",
            "Epoch [870/1000], Loss: 2712.8643\n",
            "Epoch [870/1000], Loss: 2537.6753\n",
            "Epoch [870/1000], Loss: 3201.8049\n",
            "Epoch [870/1000], Loss: 2918.085\n",
            "Epoch [870/1000], Loss: 4798.3228\n",
            "Epoch [880/1000], Loss: 2936.9131\n",
            "Epoch [880/1000], Loss: 3290.5278\n",
            "Epoch [880/1000], Loss: 2574.5347\n",
            "Epoch [880/1000], Loss: 3272.6667\n",
            "Epoch [880/1000], Loss: 2764.8694\n",
            "Epoch [880/1000], Loss: 3578.7087\n",
            "Epoch [880/1000], Loss: 3033.5605\n",
            "Epoch [880/1000], Loss: 4041.1396\n",
            "Epoch [890/1000], Loss: 3985.1086\n",
            "Epoch [890/1000], Loss: 2135.2078\n",
            "Epoch [890/1000], Loss: 3562.6814\n",
            "Epoch [890/1000], Loss: 2562.6687\n",
            "Epoch [890/1000], Loss: 3551.938\n",
            "Epoch [890/1000], Loss: 2410.1929\n",
            "Epoch [890/1000], Loss: 3336.1956\n",
            "Epoch [890/1000], Loss: 1953.2445\n",
            "Epoch [900/1000], Loss: 2908.5305\n",
            "Epoch [900/1000], Loss: 3327.0942\n",
            "Epoch [900/1000], Loss: 2682.1897\n",
            "Epoch [900/1000], Loss: 2975.9905\n",
            "Epoch [900/1000], Loss: 2722.9109\n",
            "Epoch [900/1000], Loss: 3335.3315\n",
            "Epoch [900/1000], Loss: 3356.8118\n",
            "Epoch [900/1000], Loss: 5107.0596\n",
            "Epoch [910/1000], Loss: 2630.7192\n",
            "Epoch [910/1000], Loss: 3251.7048\n",
            "Epoch [910/1000], Loss: 2728.0522\n",
            "Epoch [910/1000], Loss: 3513.4744\n",
            "Epoch [910/1000], Loss: 2527.9604\n",
            "Epoch [910/1000], Loss: 3479.8164\n",
            "Epoch [910/1000], Loss: 3161.6599\n",
            "Epoch [910/1000], Loss: 4739.314\n",
            "Epoch [920/1000], Loss: 3554.873\n",
            "Epoch [920/1000], Loss: 2045.7394\n",
            "Epoch [920/1000], Loss: 3028.7952\n",
            "Epoch [920/1000], Loss: 2853.1733\n",
            "Epoch [920/1000], Loss: 3039.5435\n",
            "Epoch [920/1000], Loss: 3657.1562\n",
            "Epoch [920/1000], Loss: 3076.636\n",
            "Epoch [920/1000], Loss: 4886.2764\n",
            "Epoch [930/1000], Loss: 4119.8354\n",
            "Epoch [930/1000], Loss: 2338.1384\n",
            "Epoch [930/1000], Loss: 2676.7981\n",
            "Epoch [930/1000], Loss: 3066.3416\n",
            "Epoch [930/1000], Loss: 3419.9033\n",
            "Epoch [930/1000], Loss: 3006.0051\n",
            "Epoch [930/1000], Loss: 2722.0239\n",
            "Epoch [930/1000], Loss: 2742.4546\n",
            "Epoch [940/1000], Loss: 3431.98\n",
            "Epoch [940/1000], Loss: 2707.1316\n",
            "Epoch [940/1000], Loss: 3574.498\n",
            "Epoch [940/1000], Loss: 3383.2744\n",
            "Epoch [940/1000], Loss: 2103.2517\n",
            "Epoch [940/1000], Loss: 3149.01\n",
            "Epoch [940/1000], Loss: 3005.8511\n",
            "Epoch [940/1000], Loss: 2023.4885\n",
            "Epoch [950/1000], Loss: 3994.97\n",
            "Epoch [950/1000], Loss: 3210.9619\n",
            "Epoch [950/1000], Loss: 3005.7366\n",
            "Epoch [950/1000], Loss: 2270.7058\n",
            "Epoch [950/1000], Loss: 3380.3069\n",
            "Epoch [950/1000], Loss: 2751.2024\n",
            "Epoch [950/1000], Loss: 2788.6311\n",
            "Epoch [950/1000], Loss: 618.9633\n",
            "Epoch [960/1000], Loss: 2683.8174\n",
            "Epoch [960/1000], Loss: 2789.8425\n",
            "Epoch [960/1000], Loss: 3477.4092\n",
            "Epoch [960/1000], Loss: 2656.855\n",
            "Epoch [960/1000], Loss: 2406.6384\n",
            "Epoch [960/1000], Loss: 3527.7349\n",
            "Epoch [960/1000], Loss: 3533.3281\n",
            "Epoch [960/1000], Loss: 5741.3154\n",
            "Epoch [970/1000], Loss: 4033.7729\n",
            "Epoch [970/1000], Loss: 2760.033\n",
            "Epoch [970/1000], Loss: 3147.1777\n",
            "Epoch [970/1000], Loss: 3141.9768\n",
            "Epoch [970/1000], Loss: 2806.6255\n",
            "Epoch [970/1000], Loss: 3337.7524\n",
            "Epoch [970/1000], Loss: 1936.9199\n",
            "Epoch [970/1000], Loss: 3599.8257\n",
            "Epoch [980/1000], Loss: 3837.1052\n",
            "Epoch [980/1000], Loss: 2093.907\n",
            "Epoch [980/1000], Loss: 3579.4675\n",
            "Epoch [980/1000], Loss: 2855.4392\n",
            "Epoch [980/1000], Loss: 2782.7224\n",
            "Epoch [980/1000], Loss: 3120.145\n",
            "Epoch [980/1000], Loss: 2768.6094\n",
            "Epoch [980/1000], Loss: 5096.2207\n",
            "Epoch [990/1000], Loss: 3109.6775\n",
            "Epoch [990/1000], Loss: 3308.1255\n",
            "Epoch [990/1000], Loss: 2638.2927\n",
            "Epoch [990/1000], Loss: 2801.5212\n",
            "Epoch [990/1000], Loss: 2778.4622\n",
            "Epoch [990/1000], Loss: 3094.7278\n",
            "Epoch [990/1000], Loss: 3183.1018\n",
            "Epoch [990/1000], Loss: 6683.8921\n",
            "Epoch [1000/1000], Loss: 2995.865\n",
            "Epoch [1000/1000], Loss: 3255.7271\n",
            "Epoch [1000/1000], Loss: 2805.0525\n",
            "Epoch [1000/1000], Loss: 3348.6499\n",
            "Epoch [1000/1000], Loss: 2304.1426\n",
            "Epoch [1000/1000], Loss: 3269.6204\n",
            "Epoch [1000/1000], Loss: 3239.8157\n",
            "Epoch [1000/1000], Loss: 1049.2426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss values that are printed every 10 epochs are tending to fluctuate around the same values, suggesting that by increasing the epochs and training time, the model may have achieved greater stability. At the very last iteration, the loss is at 1407.1082, which is an improvement from above (2988.7065).\n",
        "\n",
        "This may not appear to be a considerable difference, considering that we increased the epoch ten-fold; we may have reason to believe that the model is not learning that much more so need to also check the effect on the test data."
      ],
      "metadata": {
        "id": "KEGm4H6A8aXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "id": "_X4mpq2a9AHt",
        "outputId": "e5e643d9-5f77-431c-b2af-82f1342b5767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2867.307373046875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous MSE on the test set was ~15,525; by increasing the number of epochs to 1,000, we can see a substantial decrease in the MSE to ~2,875. So we definitely have a notable improvement in model performance on the test dataset.\n",
        "\n",
        "The reduction in the test error suggest that while the MSE previously informed us that we were not overfitting, we may have been underfitting, and the model benefitted significantly from increasing the training time. We can further experiment between epoch values between 100 and 1,000 to see whether 1,000 is actually overfitting or not, and further fine-tune the model's performance using trial-and-error."
      ],
      "metadata": {
        "id": "ympjf8c19op2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "4x2av4Rk60Xg"
      }
    }
  ]
}